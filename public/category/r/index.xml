<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Nhi (Chelsea) Le</title>
    <link>/category/r/</link>
      <atom:link href="/category/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 22 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>/category/r/</link>
    </image>
    
    <item>
      <title>Machine Learning Crash Course: Overview</title>
      <link>/post/machine-learning-crash-course-p-1-overview/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/machine-learning-crash-course-p-1-overview/</guid>
      <description>&lt;h2 id=&#34;welcome-to-my-machine-learning-crash-course&#34;&gt;&lt;strong&gt;Welcome to my Machine Learning Crash Course!&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this series, I will attempt to explain fundamental concepts in machine learning to you in understandable terms that are also not oversimplified.&lt;/p&gt;
&lt;p&gt;I am definitely not an expert in the field, just a student who has really enjoyed learning about this area. My coding journey started quite late, only a bit more than half a year ago, when I entered the second semester of my first year at university. So if you are also just starting out, believe that you can and will get better at it, so long as the passion is there.&lt;/p&gt;
&lt;p&gt;The first post is simply just an overview of what I will be covering throughout the whole crash course. 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This page&lt;/a&gt; has a really nice introduction to machine learning for beginners, which I would definitely recommend you check out first.&lt;/p&gt;
&lt;p&gt;Machine Learning, AI, Deep Learning are oftentimes confused and used interchangeably by some people. Yet they are very different areas of study:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/ml-1/index_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-22-trial-2_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Image credit: 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning for Everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The highlighted topics in the image above are those that I will touch on in future posts (since I have not learned about the remaining ones yet). There will also be additional topics to this crash course. You can find the full list below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1. Introduction to Statistical Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.1. Supervised v/s. unsupervised learning&lt;/p&gt;
&lt;p&gt;1.2. Regression v/s. classification&lt;/p&gt;
&lt;p&gt;1.3. Quality of model fit&lt;/p&gt;
&lt;p&gt;1.4. Bias-variance tradeoff&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 2. Linear Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2.1. Simple linear regression&lt;/p&gt;
&lt;p&gt;2.2. Multiple linear regression&lt;/p&gt;
&lt;p&gt;2.3. K-nearest neighbours&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 3. Classification&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3.1. Logistic regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simple model (p = 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple model (p &amp;gt; 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-class logistic regression&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.2. Linear discriminant analysis (LDA)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bayes theorem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p = 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p &amp;gt;1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quadratic discriminant analysis (QDA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.3. K-nearest neighbours&lt;/p&gt;
&lt;p&gt;3.4. Comparison of classification models&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 4. Resampling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;4.1. Cross-validation (CV)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Validation set approach&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave-one-out-cross-validation (LOOCV)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k-fold CV&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bootstrap&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 5. Regularisation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;5.1. Subset selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Best subset selection&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stepwise selection&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.2. Shrinkage methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.3. Dimension reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Principal component analysis (PCA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial least squares&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 6. Non-linearity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;6.1. Polynomial regression&lt;/p&gt;
&lt;p&gt;6.2. Step functions&lt;/p&gt;
&lt;p&gt;6.3. Basis functions&lt;/p&gt;
&lt;p&gt;6.4. Regression splines&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Piecewise polynomial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6.5. Smoothing splines&lt;/p&gt;
&lt;p&gt;6.6. Local regression&lt;/p&gt;
&lt;p&gt;6.7. Generalised additive models (GAMs)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GAMs for regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAMs for classificaiton&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 7. Tree models&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;7.1. Decision trees&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Regression trees&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification trees&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7.2. Bagging&lt;/p&gt;
&lt;p&gt;7.3. Boosting&lt;/p&gt;
&lt;p&gt;7.4. Random forests&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 8. Support Vector Machines (SVMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;8.1. Maximal margin classifier&lt;/p&gt;
&lt;p&gt;8.2. Support vector classfier&lt;/p&gt;
&lt;p&gt;8.3. SVM&lt;/p&gt;
&lt;p&gt;8.4. SVM with more than 2 classes&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 9. Unsupervised Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9.1. Clustering methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;K-mean clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hierarchical clustering&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;9.2. Principal component analysis (PCA)&lt;/p&gt;
&lt;h2 id=&#34;credit&#34;&gt;&lt;strong&gt;Credit&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I owe the majority of my machine learning knowledge and the inspiration for this course mainly to two sources:&lt;/p&gt;
&lt;h2 id=&#34;1-monash-universitys-etc3250-introduction-to-machine-learning-unit-led-by-professor-di-cook&#34;&gt;1. Monash University&amp;rsquo;s ETC3250: Introduction to Machine Learning: unit led by Professor Di Cook:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://iml.numbat.space/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://dicook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Di Cook&amp;rsquo;s website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-introduction-to-statistical-learning-with-applications-in-r-textbook-written-by-gareth-james-daniela-witten-trevor-hastie-and-robert-tibshirani&#34;&gt;2. Introduction to Statistical Learning with Applications in R: textbook written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to textbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where necessary, I will also credit additional sources of information in my future posts. Thank you for your interest in this Machine Learning Crash Course. I hope you will find it useful in some way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
