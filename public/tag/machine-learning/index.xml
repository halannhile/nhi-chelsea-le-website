<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Nhi (Chelsea) Le</title>
    <link>/tag/machine-learning/</link>
      <atom:link href="/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 23 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Machine Learning Crash Course P.1: Introduction to Machine Learning</title>
      <link>/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/</guid>
      <description>


&lt;div id=&#34;what-is-machine-learning-ml&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;What is Machine Learning (ML)?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ML is not Artificial Intelligence (AI). It is in fact a sub-category of the AI universe. &lt;span style=&#34;color:green&#34;&gt;In simple terms, ML is defined as the process of providing the computer (i.e. the machine) with data for it to learn from, so that when presented with new and unseen data in the future, it can correctly &lt;strong&gt;predict&lt;/strong&gt; a desirable response.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice the word ‘predict’ in bold here. &lt;strong&gt;Any ML task can be simply interpreted as a prediction problem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g. 1. Given all the monthly sales metrics for a particular chips brand over the past 3 years, what is the predicted number of sales for next month?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g.2. Given a handwritten number, what digit is it (from 0 to 9)?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g.3. When an email appears in your inbox, should it be classified as spam, given information such as the addresses you have blocked, advertisement words like promotion or discount, etc.?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:green&#34;&gt;In statistical terms, ML describes the process below:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given the &lt;strong&gt;input information X&lt;/strong&gt; (also called predictors, (independent) variables, features) and &lt;strong&gt;output information Y&lt;/strong&gt; (also called response or dependent variable), what is the function f that can help us arrive at Y from X, allowing for some random error?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = f(X) + \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;X variables can be either quantitative or categorical, or a combination of both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Y can be either quantitative or categorical but not both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is the random, irreducible error that we cannot control nor reduce.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of ML is to estimate unknown function f that can help make use of the predictors X’s and produce future predictions about the response Y that we are interested in.&lt;/p&gt;
&lt;p&gt;However, it is worth noting that no matter how well we try to model this function, it will always contain some error specified by &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; as defined above. So, what is the point of ML then, given that we can never get to the truth?&lt;/p&gt;
&lt;p&gt;Consider the case where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y} = \hat{f}(X)\)&lt;/span&gt; is the predicted value of unknown response Y using the known predictors X and our estimated function f. Let’s also assume that &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known. It follows that (see full derivation at the end of this post):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2 = [f(X) - \hat{f}(X)]^2 + Var(\epsilon) \space \space \space \space \space \space \space \space (1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= Reducible \space Error + Irreducible \space Error\]&lt;/span&gt;
Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reducible error is the squared difference between the actual value of Y, given by &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and the predicted value of Y, given by &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Irreducible error is the variance of the error term, &lt;span class=&#34;math inline&#34;&gt;\(Var(\epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, &lt;span style=&#34;color:green&#34;&gt;&lt;strong&gt;ML aims to estimate function f such that the reducible error is minimised.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parametric-and-non-parametric-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Parametric and Non-parametric Modeling:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To model the function f, we have two options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Parametric methods:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make assumptions about the form of f (such as: linear, quadratic, piecewise polynomial, etc.).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advantage: reduces the problem down to estimating a set of parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Disadvantage: the assumed form of function might be incorrect, resulting in poor performance. This is the problem of &lt;strong&gt;high bias&lt;/strong&gt; (to be discussed shortly).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;parametric.png&#34; alt=&#34;png&#34; /&gt;
&lt;em&gt;Image credit: &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34;&gt;ISLR&lt;/a&gt;, page 22&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Non-parametric methods:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make NO assumptions about the form of f. Instead, they try to find f that is as close to the data as possible but not perfectly close to avoid &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advantage: highly flexible and can fit a wide range of shapes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Disadvantage: Unlike parametric methods where the problem is reduced down to estimating only a set of parameters, non-parametric methods require a lot of data to accurately estimate an arbitrary f. Moreover, they are also more easily prone to &lt;strong&gt;high variance&lt;/strong&gt; or &lt;strong&gt;overfitting&lt;/strong&gt; (to be discussed shortly).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;nonparametric.png&#34; alt=&#34;png&#34; /&gt;
&lt;em&gt;Image credit: &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34;&gt;ISLR&lt;/a&gt;, page 24&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i.-supervised-and-unsupervised-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;I. Supervised and Unsupervised Learning:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Full derivation of (1):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is unknown.
We have that &lt;span class=&#34;math inline&#34;&gt;\(E(c) = c\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a constant (i.e. is known).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2 = E[(f(X)-\hat{f}(X)) + \epsilon]^2\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= E[f(X)-\hat{f}(X)]^2 + 2.E[f(X)-\hat{f}(X)].E(\epsilon) + E(\epsilon^2)\]&lt;/span&gt;
&lt;em&gt;We also made the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is random, which is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; ~ &lt;span class=&#34;math inline&#34;&gt;\(WN(0, \sigma_\epsilon^2)\)&lt;/span&gt;. Hence &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon)=0\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[=E[f(X)-\hat{f}(X)]^2 + E(\epsilon^2)\]&lt;/span&gt;
&lt;em&gt;We have: $Var() = E(^2) - [E()]^2 = E(^2) $ because &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon) = 0\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known constants:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Crash Course: Overview</title>
      <link>/post/machine-learning-crash-course-p-1-overview/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/machine-learning-crash-course-p-1-overview/</guid>
      <description>&lt;h2 id=&#34;welcome-to-my-machine-learning-crash-course&#34;&gt;&lt;strong&gt;Welcome to my Machine Learning Crash Course!&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this series, I will attempt to explain fundamental concepts in machine learning to you in understandable terms that are also not oversimplified.&lt;/p&gt;
&lt;p&gt;I am definitely not an expert in the field, just a student who has really enjoyed learning about this area. My coding journey started quite late, only a bit more than half a year ago, when I entered the second semester of my first year at university. So if you are also just starting out, believe that you can and will get better at it, so long as the passion is there.&lt;/p&gt;
&lt;p&gt;The first post is simply just an overview of what I will be covering throughout the whole crash course. 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This page&lt;/a&gt; has a really nice introduction to machine learning for beginners, which I would definitely recommend you check out first.&lt;/p&gt;
&lt;p&gt;Machine Learning, AI, Deep Learning are oftentimes confused and used interchangeably by some people. Yet they are very different areas of study:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/ml-1/index_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-22-trial-2_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Image credit: 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning for Everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The highlighted topics in the image above are those that I will touch on in future posts (since I have not learned about the remaining ones yet). There will also be additional topics to this crash course. You can find the full list below:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nhi-chelsea-le.netlify.app/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Part 1. Introduction to Statistical Learning&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1. Supervised v/s. unsupervised learning&lt;/p&gt;
&lt;p&gt;1.2. Regression v/s. classification&lt;/p&gt;
&lt;p&gt;1.3. Quality of model fit&lt;/p&gt;
&lt;p&gt;1.4. Bias-variance tradeoff&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 2. Linear Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2.1. Simple linear regression&lt;/p&gt;
&lt;p&gt;2.2. Multiple linear regression&lt;/p&gt;
&lt;p&gt;2.3. K-nearest neighbours&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 3. Classification&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3.1. Logistic regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simple model (p = 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple model (p &amp;gt; 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-class logistic regression&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.2. Linear discriminant analysis (LDA)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bayes theorem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p = 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p &amp;gt;1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quadratic discriminant analysis (QDA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.3. K-nearest neighbours&lt;/p&gt;
&lt;p&gt;3.4. Comparison of classification models&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 4. Resampling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;4.1. Cross-validation (CV)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Validation set approach&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave-one-out-cross-validation (LOOCV)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k-fold CV&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bootstrap&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 5. Regularisation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;5.1. Subset selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Best subset selection&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stepwise selection&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.2. Shrinkage methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.3. Dimension reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Principal component analysis (PCA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial least squares&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 6. Non-linearity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;6.1. Polynomial regression&lt;/p&gt;
&lt;p&gt;6.2. Step functions&lt;/p&gt;
&lt;p&gt;6.3. Basis functions&lt;/p&gt;
&lt;p&gt;6.4. Regression splines&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Piecewise polynomial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6.5. Smoothing splines&lt;/p&gt;
&lt;p&gt;6.6. Local regression&lt;/p&gt;
&lt;p&gt;6.7. Generalised additive models (GAMs)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GAMs for regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAMs for classificaiton&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 7. Tree models&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;7.1. Decision trees&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Regression trees&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification trees&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7.2. Bagging&lt;/p&gt;
&lt;p&gt;7.3. Boosting&lt;/p&gt;
&lt;p&gt;7.4. Random forests&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 8. Support Vector Machines (SVMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;8.1. Maximal margin classifier&lt;/p&gt;
&lt;p&gt;8.2. Support vector classfier&lt;/p&gt;
&lt;p&gt;8.3. SVM&lt;/p&gt;
&lt;p&gt;8.4. SVM with more than 2 classes&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 9. Unsupervised Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9.1. Clustering methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;K-mean clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hierarchical clustering&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;9.2. Principal component analysis (PCA)&lt;/p&gt;
&lt;h2 id=&#34;credit&#34;&gt;&lt;strong&gt;Credit&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I owe the majority of my machine learning knowledge and the inspiration for this course mainly to two sources:&lt;/p&gt;
&lt;h2 id=&#34;1-monash-universitys-etc3250-introduction-to-machine-learning-unit-led-by-professor-di-cook&#34;&gt;1. Monash University&amp;rsquo;s ETC3250: Introduction to Machine Learning: unit led by Professor Di Cook:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://iml.numbat.space/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://dicook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Di Cook&amp;rsquo;s website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-introduction-to-statistical-learning-with-applications-in-r-textbook-written-by-gareth-james-daniela-witten-trevor-hastie-and-robert-tibshirani&#34;&gt;2. Introduction to Statistical Learning with Applications in R: textbook written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to textbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where necessary, I will also credit additional sources of information in my future posts. Thank you for your interest in this Machine Learning Crash Course. I hope you will find it useful in some way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Recognition Kaggle Competition</title>
      <link>/project/image-recognition-project/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>/project/image-recognition-project/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Predicting hand-drawn sketches from people playing the Pictionary game.
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Machine learning models built include: Convolutional Neural Networks (93% accuracy), Random Forests, Extreme Gradient Boosting (XGBoost). 
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;The data can be found here : &lt;a href=&#34;https://github.com/googlecreativelab/quickdraw-dataset&#34;&gt;https://github.com/googlecreativelab/quickdraw-dataset&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Principle Component Analysis</title>
      <link>/project/principle-component-analysis/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/project/principle-component-analysis/</guid>
      <description>&lt;p&gt;Looking at a classification problem using linear discriminant analysis, quadratic discriminant analysis. Analysing multivariate data analysis using Principle Component Analysis for dimensionality reduction.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
