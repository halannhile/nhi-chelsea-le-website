<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Nhi (Chelsea) Le</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 23 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Machine Learning Crash Course P.1: Introduction to Machine Learning</title>
      <link>/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/</guid>
      <description>


&lt;div id=&#34;what-is-machine-learning-ml&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;What is Machine Learning (ML)?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ML is not Artificial Intelligence (AI). It is in fact a sub-category of the AI universe. &lt;span style=&#34;color:green&#34;&gt;In simple terms, ML is defined as the process of providing the computer (i.e.Â the machine) with data for it to learn from, so that when presented with new and unseen data in the future, it can correctly &lt;strong&gt;predict&lt;/strong&gt; a desirable response.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice the word â€˜predictâ€™ in bold here. &lt;strong&gt;Any ML task can be simply interpreted as a prediction problem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g. 1. Given all the monthly sales metrics for a particular chips brand over the past 3 years, what is the predicted number of sales for next month?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g.2. Given a handwritten number, what digit is it (from 0 to 9)?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E.g.3. When an email appears in your inbox, should it be classified as spam, given information such as the addresses you have blocked, advertisement words like promotion or discount, etc.?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:green&#34;&gt;In statistical terms, ML describes the process below:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given the &lt;strong&gt;input information X&lt;/strong&gt; (also called predictors, (independent) variables, features) and &lt;strong&gt;output information Y&lt;/strong&gt; (also called response or dependent variable), what is the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that can help us arrive at &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, allowing for some random error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = f(X) + \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; variables can be either quantitative or categorical, or a combination of both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be either quantitative or categorical but not both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is the random, irreducible error that we cannot control nor reduce.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of ML is to estimate unknown function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; that can help make use of the predictors &lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;s\)&lt;/span&gt; and produce future predictions about the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; that we are interested in.&lt;/p&gt;
&lt;p&gt;However, it is worth noting that no matter how well we try to model this function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, it will always contain some error specified by &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; as defined above. So, what is the point of ML then, given that we can never get to the truth?&lt;/p&gt;
&lt;p&gt;Consider the case where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y} = \hat{f}(X)\)&lt;/span&gt; is the predicted value of unknown response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using the known predictors &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and our estimated function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Letâ€™s also assume that &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known. It follows that (see full derivation at the end of this post):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2 = [f(X) - \hat{f}(X)]^2 + Var(\epsilon) \space \space \space \space \space \space \space \space (1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= Reducible \space Error + Irreducible \space Error\]&lt;/span&gt;
Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reducible error is the squared difference between the actual value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, given by &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and the predicted value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, given by &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Irreducible error is the variance of the error term, &lt;span class=&#34;math inline&#34;&gt;\(Var(\epsilon)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, &lt;span style=&#34;color:green&#34;&gt;&lt;strong&gt;ML aims to estimate function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; such that the reducible error is minimised.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parametric-and-non-parametric-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Parametric and Non-parametric Modeling:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To model the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, we have two options:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Parametric methods:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make assumptions about the form of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; (such as: linear, quadratic, piecewise polynomial, etc.).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advantage: reduces the problem down to estimating a set of parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Disadvantage: the assumed form of function might be incorrect, resulting in poor performance. This is the problem of &lt;strong&gt;high bias&lt;/strong&gt; (to be discussed shortly).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;parametric.png&#34; alt=&#34;png&#34; /&gt;
&lt;em&gt;Image credit: &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34;&gt;ISLR&lt;/a&gt;, page 22&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Non-parametric methods:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make NO assumptions about the form of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Instead, they try to find f that is as close to the data as possible but not perfectly close to avoid &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advantage: highly flexible and can fit a wide range of shapes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Disadvantage: Unlike parametric methods where the problem is reduced down to estimating only a set of parameters, non-parametric methods require a lot of data to accurately estimate an arbitrary &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Moreover, they are also prone to &lt;strong&gt;high variance&lt;/strong&gt; or &lt;strong&gt;overfitting&lt;/strong&gt; (to be discussed shortly).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;nonparametric.png&#34; alt=&#34;png&#34; /&gt;
&lt;em&gt;Image credit: &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34;&gt;ISLR&lt;/a&gt;, page 24&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which method to choose: parametric or non-parametric?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer to this question depends largely on the problem at hand, but as a rough guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When &lt;strong&gt;inference&lt;/strong&gt; is the goal, we tend to prefer &lt;strong&gt;parametric methods&lt;/strong&gt; due to their interpretability. That is, when we are interested in the relationship between the predictors &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (such as: Which predictors are related to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and to what extent? Can the relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be summarised by a linearity assumption, or is it more complicated?) parametric methods are a good choice because they make the explicit assumption about the particular form of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When &lt;strong&gt;prediction&lt;/strong&gt; is the goal, &lt;strong&gt;non-parametric methods&lt;/strong&gt; might produce more accurate results. That is, when we are only interested in obtaining as accurate predictions as possible but not in the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; - &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; relationship at all, the high flexibility of non-parametric models might be preferable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interestingly, in some cases, even when prediction is the goal, &lt;strong&gt;parametric methods&lt;/strong&gt; can prove to be more successful than their non-parametric counterparts. This might sound conflicting, but it has a lot to do with the bias-variance trade-off that will be discussed shortly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;i.-supervised-and-unsupervised-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;I. Supervised and Unsupervised Learning:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The overarching ML field can be divided into two subgroups: supervised and unsupervised learning, the definitions of which are surprisingly very simple to understand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Supervised learning:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is when we give labeled data for the machine to learn from.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In statistical terminology: in the training data, for each observation &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, there is a corresponding response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This type of learning is called â€˜supervisedâ€™ because similar to having a supervisor who will guide you in the learning process with feedback for both when you succeed or fail to do a task, the ML model will be trained on labeled data. Hence, when it practices predicting the response for the training data, it instantly knows when the prediction is correct or not thanks to the label. On the basis of this feedback, the model is further finetuned.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Unsupervised learning:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Full derivation of (1):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is unknown.
We have that &lt;span class=&#34;math inline&#34;&gt;\(E(c) = c\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a constant (i.e.Â is known).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(Y - \hat{Y}) = E[f(X) + \epsilon - \hat{f}(X)]^2 = E[(f(X)-\hat{f}(X)) + \epsilon]^2\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= E[f(X)-\hat{f}(X)]^2 + 2.E[f(X)-\hat{f}(X)].E(\epsilon) + E(\epsilon^2)\]&lt;/span&gt;
&lt;em&gt;We also made the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is random, which is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; ~ &lt;span class=&#34;math inline&#34;&gt;\(WN(0, \sigma_\epsilon^2)\)&lt;/span&gt;. Hence &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon)=0\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[=E[f(X)-\hat{f}(X)]^2 + E(\epsilon^2)\]&lt;/span&gt;
&lt;em&gt;We have: &lt;span class=&#34;math inline&#34;&gt;\(Var(\epsilon) = E(\epsilon^2) - [E(\epsilon)]^2 = E(\epsilon^2)\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon) = 0\)&lt;/span&gt;. And because &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; are known constants:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Crash Course: Overview</title>
      <link>/post/machine-learning-crash-course-p-1-overview/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/machine-learning-crash-course-p-1-overview/</guid>
      <description>&lt;h2 id=&#34;welcome-to-my-machine-learning-crash-course&#34;&gt;&lt;strong&gt;Welcome to my Machine Learning Crash Course!&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this series, I will attempt to explain fundamental concepts in machine learning to you in understandable terms that are also not oversimplified.&lt;/p&gt;
&lt;p&gt;I am definitely not an expert in the field, just a student who has really enjoyed learning about this area. My coding journey started quite late, only a bit more than half a year ago, when I entered the second semester of my first year at university. So if you are also just starting out, believe that you can and will get better at it, so long as the passion is there.&lt;/p&gt;
&lt;p&gt;The first post is simply just an overview of what I will be covering throughout the whole crash course. 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This page&lt;/a&gt; has a really nice introduction to machine learning for beginners, which I would definitely recommend you check out first.&lt;/p&gt;
&lt;p&gt;Machine Learning, AI, Deep Learning are oftentimes confused and used interchangeably by some people. Yet they are very different areas of study:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/ml-1/index_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-22-trial-2_files/ml-1-overview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Image credit: 
&lt;a href=&#34;https://vas3k.com/blog/machine_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning for Everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The highlighted topics in the image above are those that I will touch on in future posts (since I have not learned about the remaining ones yet). There will also be additional topics to this crash course. You can find the full list below:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nhi-chelsea-le.netlify.app/post/machine-learning-crash-course-p-1-introduction-to-statistical-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Part 1. Introduction to Statistical Learning&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1. Supervised v/s. unsupervised learning&lt;/p&gt;
&lt;p&gt;1.2. Regression v/s. classification&lt;/p&gt;
&lt;p&gt;1.3. Quality of model fit&lt;/p&gt;
&lt;p&gt;1.4. Bias-variance tradeoff&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 2. Linear Regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2.1. Simple linear regression&lt;/p&gt;
&lt;p&gt;2.2. Multiple linear regression&lt;/p&gt;
&lt;p&gt;2.3. K-nearest neighbours&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 3. Classification&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3.1. Logistic regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simple model (p = 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple model (p &amp;gt; 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-class logistic regression&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.2. Linear discriminant analysis (LDA)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bayes theorem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p = 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA with p &amp;gt;1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quadratic discriminant analysis (QDA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3.3. K-nearest neighbours&lt;/p&gt;
&lt;p&gt;3.4. Comparison of classification models&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 4. Resampling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;4.1. Cross-validation (CV)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Validation set approach&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave-one-out-cross-validation (LOOCV)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k-fold CV&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bootstrap&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 5. Regularisation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;5.1. Subset selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Best subset selection&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stepwise selection&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.2. Shrinkage methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ridge regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lasso&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5.3. Dimension reduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Principal component analysis (PCA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial least squares&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 6. Non-linearity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;6.1. Polynomial regression&lt;/p&gt;
&lt;p&gt;6.2. Step functions&lt;/p&gt;
&lt;p&gt;6.3. Basis functions&lt;/p&gt;
&lt;p&gt;6.4. Regression splines&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Piecewise polynomial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6.5. Smoothing splines&lt;/p&gt;
&lt;p&gt;6.6. Local regression&lt;/p&gt;
&lt;p&gt;6.7. Generalised additive models (GAMs)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GAMs for regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GAMs for classificaiton&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 7. Tree models&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;7.1. Decision trees&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Regression trees&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification trees&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7.2. Bagging&lt;/p&gt;
&lt;p&gt;7.3. Boosting&lt;/p&gt;
&lt;p&gt;7.4. Random forests&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Part 8. Support Vector Machines (SVMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;8.1. Maximal margin classifier&lt;/p&gt;
&lt;p&gt;8.2. Support vector classfier&lt;/p&gt;
&lt;p&gt;8.3. SVM&lt;/p&gt;
&lt;p&gt;8.4. SVM with more than 2 classes&lt;/p&gt;
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Part 9. Unsupervised Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9.1. Clustering methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;K-mean clustering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hierarchical clustering&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;9.2. Principal component analysis (PCA)&lt;/p&gt;
&lt;h2 id=&#34;credit&#34;&gt;&lt;strong&gt;Credit&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I owe the majority of my machine learning knowledge and the inspiration for this course mainly to two sources:&lt;/p&gt;
&lt;h2 id=&#34;1-monash-universitys-etc3250-introduction-to-machine-learning-unit-led-by-professor-di-cook&#34;&gt;1. Monash University&amp;rsquo;s ETC3250: Introduction to Machine Learning: unit led by Professor Di Cook:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://iml.numbat.space/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Course website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://dicook.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Di Cook&amp;rsquo;s website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-introduction-to-statistical-learning-with-applications-in-r-textbook-written-by-gareth-james-daniela-witten-trevor-hastie-and-robert-tibshirani&#34;&gt;2. Introduction to Statistical Learning with Applications in R: textbook written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to textbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where necessary, I will also credit additional sources of information in my future posts. Thank you for your interest in this Machine Learning Crash Course. I hope you will find it useful in some way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing technical content in Academic</title>
      <link>/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Academic.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the &lt;code&gt;highlight&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$...$&lt;/code&gt; or &lt;code&gt;$$...$$&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left |\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right |^2}$$&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$\nabla F(\mathbf{x}_{n})$&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the &lt;code&gt;\\&lt;/code&gt; math linebreak:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \&lt;br&gt;
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$&lt;/p&gt;
&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;
&lt;p&gt;You can even write your todo lists in Academic too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- [x] Write math example
- [x] Write diagram example
- [ ] Do something else
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write math example&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write diagram example&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Do something else&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;
&lt;p&gt;Represent your data in tables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;| First Header  | Second Header |
| ------------- | ------------- |
| Content Cell  | Content Cell  |
| Content Cell  | Content Cell  |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;First Header&lt;/th&gt;
&lt;th&gt;Second Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;asides&#34;&gt;Asides&lt;/h3&gt;
&lt;p&gt;Academic supports a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#alerts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcode for asides&lt;/a&gt;, also referred to as &lt;em&gt;notices&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;. By wrapping a paragraph in &lt;code&gt;{{% alert note %}} ... {{% /alert %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% alert note %}}
A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
{{% /alert %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;icons&#34;&gt;Icons&lt;/h3&gt;
&lt;p&gt;Academic enables you to use a wide range of 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/#icons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;icons from &lt;em&gt;Font Awesome&lt;/em&gt; and &lt;em&gt;Academicons&lt;/em&gt;&lt;/a&gt; in addition to 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#emojis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emojis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some examples using the &lt;code&gt;icon&lt;/code&gt; shortcode to render icons:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; icon name=&amp;quot;terminal&amp;quot; pack=&amp;quot;fas&amp;quot; &amp;gt;}} Terminal  
{{&amp;lt; icon name=&amp;quot;python&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} Python  
{{&amp;lt; icon name=&amp;quot;r-project&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} R
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-terminal  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Terminal&lt;br&gt;

  &lt;i class=&#34;fab fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Python&lt;br&gt;

  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; R&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it ðŸ™Œ&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (
&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
